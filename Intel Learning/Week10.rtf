{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Due to the hardware needs of this lesson, after trial and error I switched to google collab to use their TPU backend to train the models. While these models are extremely interesting and I would like to investigate them further at a later time, for the purposes of using FPGA\'92s to examine speed and resource management for Deep Learning, these language models seem to be too involved to be the main focus. \
The Google Colab notebook can be found here : https://colab.research.google.com/drive/1jE-PH_aW6C0B56Rs7hWwn05Ak7HZmuv2#scrollTo=UvrQ3GN1gfQO\
\
\
}